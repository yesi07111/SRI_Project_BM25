# ğŸ“š Proyecto de RecuperaciÃ³n de InformaciÃ³n con Okapi BM25 
## ğŸš€ DescripciÃ³n General  
Sistema de recuperaciÃ³n de informaciÃ³n implementando el modelo probabilÃ­stico **BM25** con mejoras en preprocesamiento lingÃ¼Ã­stico. EvalÃºa relevancia documento-consulta mediante distribuciÃ³n estadÃ­stica de tÃ©rminos con parÃ¡metros ajustables (`kâ‚`, `b`).  

CaracterÃ­sticas clave:  
- âœ… LematizaciÃ³n avanzada con Spacy  
- âœ… NormalizaciÃ³n semÃ¡ntica mediante embeddings  
- âœ… CÃ¡lculo automÃ¡tico de mÃ©tricas TF/IDF  
- âœ… EvaluaciÃ³n integrada contra qrels oficiales

## ğŸ”§ Componentes TÃ©cnicos  
### ğŸ§  Modelo MatemÃ¡tico  
```python
Score(D,Q) = Î£â‚œâˆˆQ [IDF(t) Â· (F(t,D)Â·(kâ‚+1))/(F(t,D)+kâ‚(1 - b + bÂ·|D|/avgdl))]
```
Donde:
- **IDF(t)** (*Inverse Document Frequency*):  
  `log[(N - nâ‚œ + 0.5)/(nâ‚œ + 0.5) + 1]`  
  - *N*: Total de documentos en el corpus  
  - *nâ‚œ*: Documentos que contienen el tÃ©rmino *t*  
  - **FunciÃ³n**: Penaliza tÃ©rminos comunes y premia los raros (ej: "el" vs "neurociencia").  

- **F(t,D)** (*Term Frequency*):  
  - Frecuencia cruda del tÃ©rmino *t* en el documento *D*.  

- **kâ‚**:  
  - Controla la **saturaciÃ³n de TF** (ej: si kâ‚â†’âˆ, TF no se satura â†’ repeticiones extremas ganan peso).  
  - Valor tÃ­pico: 1.2-2.0. Alto kâ‚ = mayor influencia de repeticiones.  

- **b**:  
  - Controla la **normalizaciÃ³n por longitud de documento** (0 = desactivada, 1 = mÃ¡xima normalizaciÃ³n).  
  - **|D|/avgdl**: Ratio entre longitud del documento y longitud promedio del corpus.  

- **Denominador**:  
  `F(t,D) + kâ‚(1 - b + bÂ·|D|/avgdl)`  
  - Equilibra TF con penalizaciÃ³n por documentos largos (evita bias hacia textos extensos).  

### ğŸ“Œ Marco de Relevancia ProbabilÃ­stica (PRF)  
- **HipÃ³tesis central**:  
  "La relevancia de un documento *D* a una consulta *Q* puede estimarse mediante la distribuciÃ³n estadÃ­stica de sus tÃ©rminos".  

- **Supuestos clave**:  
  1. **Independencia entre tÃ©rminos**: BM25 asume tÃ©rminos no correlacionados (modelo "bag-of-words").  
  2. **Relevancia binaria**: Documentos son relevantes o no (aunque el score es continuo).  
  3. **DistribuciÃ³n no uniforme**: TÃ©rminos relevantes tienden a agruparse en documentos relevantes.  

### ğŸ”„ DinÃ¡mica TF-IDF Mejorada  
- **TF clÃ¡sico**: Lineal (ej: 5 repeticiones = 5Ã— peso).  
- **TF en BM25**: **FunciÃ³n de saturaciÃ³n** (ej: 5 repeticiones â‰  5Ã—, sino log(5) â‰ˆ 1.6Ã—).  
  - **Ventaja**: Mitiga spam de repeticiones (ej: "viagra viagra viagra...").  

### ğŸ› ï¸ Pipeline de Procesamiento  
1. **Preprocesamiento**:  
   - ğŸ”  ConversiÃ³n a minÃºsculas  
   - ğŸ” LematizaciÃ³n morfolÃ³gica  
   - ğŸ—‘ï¸ Filtrado stopwords/puntuaciÃ³n  
2. **Indexado**:  
   - ğŸ“Š CÃ¡lculo TF-IDF mejorado  
   - ğŸ“ NormalizaciÃ³n por avgdl  
3. **BÃºsqueda**:  
   - ğŸ” CÃ¡lculo scores BM25  
   - ğŸ† Ranking automÃ¡tico Top-K

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n  
```python
{
  'all_relevant': Set[doc_ids],    # Documentos realmente relevantes
  'all_retrieved': Set[doc_ids],   # Documentos recuperados
  'relevant_retrieved': Set[doc_ids]  # IntersecciÃ³n relevante
}
```

## ğŸŒ± EvoluciÃ³n HistÃ³rica del Modelo  
### 1. Modelo Booleano (1Â° GeneraciÃ³n)  
- Relevancia binaria: Documentos cumplen operadores lÃ³gicos (AND/OR/NOT).  
- **LimitaciÃ³n**: Sin ranking ni ponderaciÃ³n.  

### 2. Modelo Vectorial (TF-IDF)  
- Introduce ranking continuo basado en similitud coseno.  
- **Problema**: TF lineal y normalizaciÃ³n rÃ­gida.  

### 3. Binary Independence Model (BIM)  
- Primer modelo probabilÃ­stico. Usa P(relevancia | tÃ©rminos).  
- **Fallo**: Ignora TF y longitud de documentos.  

### 4. BM25 (Okapi, 1994)  
- **Innovaciones**:  
  - SaturaciÃ³n no lineal de TF.  
  - NormalizaciÃ³n adaptable por longitud (parÃ¡metro *b*).  
  - IDF robusto contra tÃ©rminos ultra-frecuentes.
 
## ğŸ’ª Ventajas Cualitativas de BM25  
### ğŸ›¡ï¸ Robustez Operativa  
- **Adaptabilidad**: ParÃ¡metros *kâ‚* y *b* permiten ajuste fino a dominios especÃ­ficos (ej: *b=0.8* para blogs largos vs *b=0.3* para tuits).  

### ğŸ§© Balance LÃ©xico-Estructural  
- **Evita overfitting**:  
  - TF saturado â†’ Reduce impacto de spam lÃ©xico.  
  - NormalizaciÃ³n de longitud â†’ Neutraliza ventajas artificiales de documentos largos.  

### âš¡ Eficiencia Computacional  
- **Costo O(n)**: Escala linealmente con tamaÃ±o del corpus (vs modelos neuronales O(nÂ²)).  

### ğŸŒ GeneralizaciÃ³n EmpÃ­rica  
- **Dominio-agnÃ³stico**: Funciona bien en corpus heterogÃ©neos (cientÃ­ficos, legales, web) sin reentrenamiento.

## âš ï¸ Limitaciones Conceptuales  
- **Ceguera semÃ¡ntica**: Ignora sinÃ³nimos y relaciones contextuales (ej: "auto" â‰  "coche").  
- **Estaticidad**: No aprende de interacciones usuario (requiere reindexar para actualizar relevancia).  
- **Dependencia de preprocesamiento**: La calidad del lematizador/stopwords afecta resultados drÃ¡sticamente.  

## ğŸ“š Referencia BibliogrÃ¡fica  
```bibtex
@article{robertson2004probabilistic,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and Taylor, Michael},
  journal={Foundations and Trends in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2004},
  doi={10.1561/1500000019}
}
```


## ğŸ’» Uso BÃ¡sico  
```python
# InicializaciÃ³n
model = InformationRetrievalModel(k1=1.5, b=0.75)
model.fit('trec-robust04')  # Carga dataset

# BÃºsqueda y evaluaciÃ³n
results = model.predict(top_k=10)
metrics = model.evaluate()
```
